<!DOCTYPE html>

<html lang="en" xmlns="http://www.w3.org/1999/xhtml">
<head>
    <meta charset="utf-8" />
    <title>Representing Semantics</title>
</head>
<body>
    <h1>“Colourless green ideas sleep furiously”: My approach to Formalising Semantics in Computational Linguistics</h1>
    <p>The principal objective of this module for me is to establish a foundation in research into formalising text into a data-structure to capture its meaning so that others can use what I’ve surveyed and deduced myself, then build upon it to ascend up this Mount Everest of an end goal which is to create a formalisation of semantics similar to how ‘The Sims’ formalises life in the real world.</p>
    <p>The phrase ‘formalising text into a data-structure’ seems quite straightforward initially; shapes have been formalised like that thanks to research into Computer Vision so surely this can be repeated for text. 
    However, Computer Vision and Graphics have been researched extensively; it is a mature field, whereas my research indicates that considerably less scholars have invested time into this aspect of Natural Language Understanding.</p>
    <p>This blog describes my approaches, both successful and unsuccessful, to gathering information on a problem that Professor Teufel (University of Cambridge) describes as far from being solved; 
    “The second wave of AI, right now, is soon going to fail because too much trickery and even self-trickery is used.”</p>
    <h2>APPROACHES TO RESEARCH</h2>
    <p>From the perspective of a first-year student such as myself, research papers are a nightmare. Between presumed understanding of advanced concepts in mathematics and descriptions of ideas that span several pages, distinguishing between valuable information and material of no relevance to me was always going to be challenging. 
    In order to face that problem however, you need to acquire relevant research papers. This was the painfully arduous at times.</p>
    <p>I started with keyword searching which could be considered a ‘lottery-based approach’ because while you are likely to find some relevant papers which include the terminology that you use, it’s possible that there are other insightful papers which are omitted from the search results because they address the same problem differently. 
    I would actively discourage other researches from doing this for a long time because you’re unlikely to gather anything meaningful apart from the names of important people from the field of research. This in itself constitutes as something of value because it leads us directly to our next research approach: searching by people who have made a contribution to the field.</p>
    <p>Keyword searching for phrases like ‘formalising text into data structure’ and ‘rule-based approach to formalising text’ yielded a limited supply of papers to work with. 
    ‘Formalising natural language’ was surprisingly successful because I read a paper which lead me on to researching tools from Computational Linguistics. It described 4 tools for formalising grammars; XFST, GPSG, LFG and HPSG. Initially, I was hesitant to accept that researching linguistics papers would be of any value. However, I reminded myself that formalising text into a data structure is a problem which applies Machine Learning to a linguistics field of research.</p>
    <p>I missed countless results that are related representing text by using the keywords selected above; they did not account for graph-based representations of the meaning of text which from my perspective, is a more semantically-oriented approach to formalising text.</p>
    <p>TLDR: Use keyword searches and visit conference websites to identify those who have made valuable contributions to Computational Linguistics</p>
    <h1>FIGURES OF INTEREST: NOAM CHOMSKY</h1>
    <img src="https://www.google.com/url?sa=i&url=https%3A%2F%2Fwww.dw.com%2Fen%2Fdissident-intellectual-noam-chomsky-at-90%2Fa-46629642&psig=AOvVaw1HCFJpugCwxixbgO1PmXkN&ust=1583617975737000&source=images&cd=vfe&ved=0CAIQjRxqFwoTCNCqk-vqhugCFQAAAAAdAAAAABAJ"/>
    <p>The first important name I came across while researching Linguistics is Avram Noam Chomsky (1928 – Present). In 1957, he published a monograph called ‘Syntactic Structures’ which argues that natural language semantics and syntax should be considered independent of each other; sentences which are syntactically coherent can have no meaning. 
    His famous example is ‘Colourless green ideas sleep furiously’. The syntactic structure [ADJ, ADJ, NOUN, VERB, ADV] is acceptable yet the individual words when put together are nonsense. </p>
    <p>One valuable point that I deduced from reading about “Syntactic Structures” is that a metric for success for our working machine is for it to be able to recognise nonsense. The ability to do that would indicate to me that the machine is clever; it uses reasoning and understands each of the words in that sentence as well as why they make no sense in that order. 
    A true measure of this intelligence would be if the machine could explain why that combination of words is invalid. In our example case, ‘Colourless green ideas sleep furiously’, a well-designed machine would argue [‘Something cannot be colourless and coloured at the same time.’, ‘Sleep is a passive state, so adverbs related to aggression don’t make sense’] just as a human would.</p>
    <p>Furthermore, Chomsky distinguishes between a ‘grammatical’ and ‘ungrammatical’ sentences where ‘grammatical’ sentences are intuitively acceptable to a native speaker. His famous example is grammatical, but not statistically probable. He then relates phonology to semantics, arguing that how a word is said means more than the word itself. </p>
    <h1>FIGURES OF INTEREST: CHRISTOPHER D. MANNING</h1>
    <img src="https://www.google.com/url?sa=i&url=https%3A%2F%2Fnlp.stanford.edu%2Fmanning%2F&psig=AOvVaw0hLvuZuB1NBdMYUJeWwKw-&ust=1583617942921000&source=images&cd=vfe&ved=0CAIQjRxqFwoTCLCFyNrqhugCFQAAAAAdAAAAABAD"/>
    <p>Professor Manning lectures and researches Natural Language Processing at the Stanford University. His focus is on a computational linguistics approach to parsing, Natural Language Inference and Multilingual Language Processing. In fact, he was a key developer in the SNLI corpus.</p>
    <a href="https://nlp.stanford.edu/pubs/snli_paper.pdf">The paper describing the process of creation this corpus </a>
    <p>Natural Language Inference is tThe task of determining if a hypothesis is true, undetermined or false given a premise. A metric for success for this is if a machine can determine the nature of the hypothesis having been shown only the premise and in addition, demonstrate ‘common sense’ by explaining why it has reached the decision. 
    The reason that we are concerned with NLI is because our final product will likely be trained on the rules of the world so that it can take input like a movie synopsis, then reproduce an accurate simulation of what was described in it. Another student on my course is working on Automated Text Generation, so one of my visions would be for their tool to produce a novel synopsis, then for mine to understand the situation it describes and generate a simulation to represent it. 
    If you are interested in this vision, I recommend that you read about GANs below.</p>
    <h1>What is a GAN and why do they matter to us?</h1>
    <p>In computer vision, a Generative Adversarial Network has a generator and a discriminator. The role of the discriminator is to recognise an object and state how certain it is that it is correct (1 being “THIS IS THE OBJECT” and 0 being “THIS IS NOT THE OBJECT”. The role of the generator is to make new images of that object. In a test, the discriminator can be shown a mix of images of a given object (some real, some generated) and at the beginning of the generator training, the discriminator should be able to easily identify the generated images because they’ll simply not be like the real object. 
    However, as the generator improves, the certainties from the discriminator should move closer to 0.5 (“UNSURE IF THIS IS OR ISN’T THE OBJECT”). Once the discriminator reaches 0.5 consistently, we can discard it because we have made a generator that can make images which are very similar to the real object. When I have time, I’m going to see if anything like that has been done for text-based resources. 
    It would be very interesting if a discrimination tool could be/has been designed to recognise automatically generated text based on a characteristic such as writing form. As far as I’m aware, this task is similar to our end-goal of formalising text in that right now, there is a computer vision version but no NLU version.</p>
    <p>TLDR: It is valuable to do what has been done for shapes in Computer Vision, but for text.<p>
    <h1>FIGURES OF INTEREST: KRISHNA P. GUMMADI</h1>
    <img src="https://www.bing.com/images/search?q=krishna%2Bgummadi&cbir=ms&rxc=12&sbirxc=30&mid=9AB8BF23C89529730394D31D7D9844A7ABAFC109&simid=0&vw=4ccd3%20661f7%201f52d%206fb7b%2049e15%208a4ac%2049e22%203a476%2043e6a%2009f2e%20c29ca%2084770%20c47ff%20d1bfa%203e1b8%20800da%2028e0c%2027daa%208e4a8%201f38f%20350c1b02596b58a3e4fdf2d81017118a4758f6b69f0cd42028b21e44250231035b02d22f041d19161d2d4c55b8bbde5a19a18f77d9409d9ffc7b12c035700d0259af8a64147537150cbf85ad72a28ddfb6455428d8baf8c36d181061cd546bd8143cb8e36f68fccd18cb18591c60c8b6cd017c1827dcf1ca90437163ae5b2b2d7669825485aceecf1c51d4c3d0d0a432e01cb17d882d&FORM=IMSFRD"/>
    <p>Professor Gummadi researches at the Max Planck institute and is very interested in the ethics of AI which is used to make decisions which affect people. He deals with AI that make decisions about ‘grey-areas’ like predictive policing. In the Turing Talk 2019, he highlighted startling biases within machine learning systems like facial recognition software and a recidivation predictor called COMPAS. 
    Some facial recognition tools have a success rate as low as 66% for black females despite being 99% accurate when identifying white males. Our end goal also relies on unbiased decision making not only founded in rule-based logic but also in common sense given how ambiguous the English language can be. If our machine is trained on biased data, its common sense will be warped and the simulations we produce may not be an objective representation of what was actually described.</p>
    <p>He also highlights bias within an existing semantic representation: Word2Vec which shows the translation between ‘Man’ and ‘Computer’ as of the same magnitude of that between ‘Woman’ and ‘Housekeeper’.</p>
    <p>Gummadi is also interested in multilinguistic semantics in the form of machine translation. This field could be bolstered by an investigation into representing semantics because of how many phrases do not semantically translate from source to target language; “Tengo un hambre que da calambre.” literally translates to “I’ve a hunger that gives cramps” but is semantically equivalent to “I’m starving.” 
    Making a machine understand the concepts that are represented by words will inevitably lead to improved translation quality and would mean that our end-goal could reproduce the same simulation regardless of the language of the input text.</p>
    <h1>FIGURES OF INTEREST: SEBASTIAN PADÓ</h1>
    <p>I found this researcher because he is a keynote speaker for RANLP which took place in Bulgaria during Summer 2019. His expertise lies in Computational Linguistics; having carried out postdoctoral research in Stanford. His research interests specifically address our ‘Everest’: developing graphical representations for the meaning of natural language words, phrases and documents obtained from a corpora.</p>
    <h1>WHY IS REPRESENTING THE MEANING OF WORDS IMPORTANT?</h1>
    <p>The value of representing the meaning of words could be described anecdotally in terms of a band playing a song; your interpretation of a song is highest when all instruments are playing together. 
    If you only listen to one instrument, your understanding of the overall song is greatly reduced. The same is true in NLU; if you isolate a word from its context, it is semantically ambiguous. Although context is key, syntactic ambiguity is another challenge that an intelligent machine would have to overcome. Take the Winograd Schema; a list of 150 sentences containing a referent (it) and accompanied by a couple of words, each changing what the referent refers to. 
    If the machine can score highly on Winograd Schema examples, it’s a sign that it has common sense and contextual awareness.</p>
    <h1>REQUIREMENT FOR FORMALISING TEXT AS A DATASTRUCTURE: COMPUTATIONAL POWER</h1>
    <p>
        In the paper ‘Language Modelling with Gated Convolutional Networks’, Dauphin, Fan, Auli and Grangier propose that a metric of success for a model. The throughput is the number of tokens being processed per unit time. Parallel processing accelerates this. Responsiveness is described as processing tokens in sequence. Typically,  responsiveness and throughput are inversely proportional however batching can make them directly proportional to each other.
    </p>
    <a href="https://arxiv.org/pdf/1612.08083.pdf">You can read the full paper here.</a>
    <h1>VISION FOR THE FUTURE: DESCRIBING MY ‘EVEREST’</h1>
    <p>During my research, I was alarmed at the lack of semantic formalisation tools I came across which is a big surprise because simulations based on text which has been formalised based on its meaning could be applied to legal scripts to model scenarios from an objective and unbiased perspective, provided we overcome the biases described in this blog during training.</p>
    <p>My aim for the next month is to work on setting up a controlled example case which models a simple supervised situation which has been well described, then focus on applying a GTP-2 approach to the model, letting it go unsupervised to see how intelligent it is with no guidance. GPT-2 performed surprisingly well because of the 40GB of raw text that it was trained on, ending up with a 1.5 billion parameter model.
    The simple simulation that I run is likely to be two syntactically different sentences which are semantically identical and show the same simulation; “The thief stole the computer”, “The burglar took the PC without permission” for example. If my controlled model is built upon the research that I’ve carried out, then it’s evidence that the mentality of ‘resilience combined with technical fearlessness’ that this module tries to ingrain into its students is the key to standing out and making something of lasting value to others.</p>
</body>
</html>